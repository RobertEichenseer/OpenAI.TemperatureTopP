{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI - Demystifying Temperature and TopP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "Both Temperature and TopP influence the diversity of responses from LLM models. LLMs aren't deterministic systems, meaning return values can differ even if the same prompt is provided to the LLM. \n",
    "\n",
    "Let's see this behavior in an example using Powershell and curl:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call Azure Open AI chat API endpoint:\n",
    "\n",
    "The first call uses the following parameter:\n",
    "\n",
    "- **System Message:** \"You are an AI assistant that completes statements and phrases. You just finish the provided statement!\n",
    "- **User:** \"Once upon a time\"\n",
    "- **Temperature:** Set to 1 indicating to the model to be \"creative\" with responses.\n",
    "\n",
    "P.S.: The necessary Azure environment (Azure OpenAI, model deployment etc.) can be created using the provided [Azure CLI script](../CreateEnv/CreateEnv.azcli).The API endpoint, API key and model deployment name to run curl are provided in environment variables:\n",
    "\n",
    "```azurecli\n",
    "$ENV:AZURE_OPENAI_ENDPOINT = $csEndpoint\n",
    "$ENV:AZURE_OPENAI_API_KEY = $csApiKey\n",
    "$ENV:AZURE_OPENAI_DEPLOYMENTNAME = $modelDeploymentName\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "dotnet_interactive": {
     "language": "pwsh"
    },
    "polyglot_notebook": {
     "kernelName": "pwsh"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31;1m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\u001b[0m\n",
      "\u001b[31;1m                                 Dload  Upload   Total   Spent    Left  Speed\u001b[0m\n",
      "\u001b[31;1m\u001b[0m\n",
      "\u001b[31;1m  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\u001b[0m\n",
      "\u001b[31;1m100   742  100   364  100   378    308    320  0:00:01  0:00:01 --:--:--   630\u001b[0m\n",
      "\u001b[31;1m100   742  100   364  100   378    308    320  0:00:01  0:00:01 --:--:--   630\u001b[0m\n",
      "in a faraway land, there lived a handsome prince who was cursed by an evil sorceress\n",
      "\u001b[31;1m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\u001b[0m\n",
      "\u001b[31;1m                                 Dload  Upload   Total   Spent    Left  Speed\u001b[0m\n",
      "\u001b[31;1m\u001b[0m\n",
      "\u001b[31;1m  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\u001b[0m\n",
      "\u001b[31;1m100   745  100   367  100   378    345    355  0:00:01  0:00:01 --:--:--   703\u001b[0m\n",
      "\u001b[31;1m100   745  100   367  100   378    344    355  0:00:01  0:00:01 --:--:--   702\u001b[0m\n",
      ", there was a beautiful princess who lived in a magnificent castle at the top of a hill\n",
      "\u001b[31;1m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\u001b[0m\n",
      "\u001b[31;1m                                 Dload  Upload   Total   Spent    Left  Speed\u001b[0m\n",
      "\u001b[31;1m\u001b[0m\n",
      "\u001b[31;1m  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\u001b[0m\n",
      "\u001b[31;1m100   716  100   338  100   378    370    413 --:--:-- --:--:-- --:--:--   785\u001b[0m\n",
      "\u001b[31;1m100   716  100   338  100   378    370    413 --:--:-- --:--:-- --:--:--   785\u001b[0m\n",
      "there was a beautiful princess who lived in a grand castle\n"
     ]
    }
   ],
   "source": [
    "$apiEndpoint = \"provide your API endpoint\"\n",
    "$apiKey = \"provide your API key\"\n",
    "$deploymentName = \"provide your deployment name\"\n",
    "\n",
    "\n",
    "$url = \"$apiEndpoint/openai/deployments/$deploymentName/chat/completions?api-version=2023-03-15-preview\"\n",
    "\n",
    "$jsonPayload = @\"\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are an AI assistant that completes statements and phrases. You just finish the provided statement!\"\n",
    "        }, \n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Once upon a time\" \n",
    "        }\n",
    "    ], \n",
    "    \"max_tokens\": 800,\n",
    "    \"temperature\": 1,\n",
    "    \"stop\": [\".\"]\n",
    "}\n",
    "\"@\n",
    "\n",
    "for ($i=1; $i -le 3; $i++) {\n",
    "    $response = curl $url `\n",
    "    -H \"Content-Type: application/json\" `\n",
    "    -H \"api-key: $apiKey\" `\n",
    "    -d $jsonpayload \n",
    "    \n",
    "    Write-Host ($response | ConvertFrom-Json).choices.message.content \n",
    "}\n",
    "\n",
    "#!set --value @pwsh:apiEndpoint --name apiEndpoint\n",
    "#!set --value @pwsh:apiKey --name apiKey\n",
    "#!set --value @pwsh:deploymentName --name deploymentName\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different responses are created. GPT models are trained on large and diverse datasets. Meaning there are plenty of possible completions to the above simple ***Once upon a time*** user interaction. Hence the different responses from the LLM.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature & TopP\n",
    "\n",
    "By providing Temperature and/or TopP to the model the variability of responses can be influenced. Let's first have a look to Temperature\n",
    "\n",
    "### Temperature\n",
    "\n",
    "Temperature is a float value with a range between 0 and 1. Where 0 indicates to the model to be more deterministic meaning less variable response should be created. 1 indicates to the model that it can respond with more \"creativity\" and be less deterministic.\n",
    "\n",
    "Let's re-run the example with a Temperature of 0 to indicate to the model to be more deterministic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "dotnet_interactive": {
     "language": "pwsh"
    },
    "polyglot_notebook": {
     "kernelName": "pwsh"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31;1m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\u001b[0m\n",
      "\u001b[31;1m                                 Dload  Upload   Total   Spent    Left  Speed\u001b[0m\n",
      "\u001b[31;1m\u001b[0m\n",
      "\u001b[31;1m  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\u001b[0m\n",
      "\u001b[31;1m  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\u001b[0m\n",
      "\u001b[31;1m100   776  100   398  100   378    295    280  0:00:01  0:00:01 --:--:--   575\u001b[0m\n",
      "in a far-off land, there lived a brave knight who embarked on a perilous quest to save the kingdom from an evil dragon\n",
      "\u001b[31;1m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\u001b[0m\n",
      "\u001b[31;1m                                 Dload  Upload   Total   Spent    Left  Speed\u001b[0m\n",
      "\u001b[31;1m\u001b[0m\n",
      "\u001b[31;1m  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\u001b[0m\n",
      "\u001b[31;1m100   378    0     0  100   378      0    276  0:00:01  0:00:01 --:--:--   277\u001b[0m\n",
      "\u001b[31;1m100   776  100   398  100   378    290    275  0:00:01  0:00:01 --:--:--   567\u001b[0m\n",
      "in a far-off land, there lived a brave knight who embarked on a perilous quest to save the kingdom from an evil dragon\n",
      "\u001b[31;1m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\u001b[0m\n",
      "\u001b[31;1m                                 Dload  Upload   Total   Spent    Left  Speed\u001b[0m\n",
      "\u001b[31;1m\u001b[0m\n",
      "\u001b[31;1m  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\u001b[0m\n",
      "\u001b[31;1m100   378    0     0  100   378      0    274  0:00:01  0:00:01 --:--:--   275\u001b[0m\n",
      "\u001b[31;1m100   776  100   398  100   378    273    259  0:00:01  0:00:01 --:--:--   534\u001b[0m\n",
      "in a far-off land, there lived a brave knight who embarked on a perilous quest to save the kingdom from an evil dragon\n"
     ]
    }
   ],
   "source": [
    "$jsonPayload = @\"\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are an AI assistant that completes statements and phrases. You just finish the provided statement!\"\n",
    "        }, \n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Once upon a time\" \n",
    "        }\n",
    "    ], \n",
    "    \"max_tokens\": 800,\n",
    "    \"temperature\": 0,\n",
    "    \"stop\": [\".\"]\n",
    "}\n",
    "\"@\n",
    "\n",
    "for ($i=1; $i -le 3; $i++) {\n",
    "    $response = curl $url `\n",
    "    -H \"Content-Type: application/json\" `\n",
    "    -H \"api-key: $apiKey\" `\n",
    "    -d $jsonpayload \n",
    "    \n",
    "    Write-Host ($response | ConvertFrom-Json).choices.message.content \n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen above Temperature controls the variability of the models responses based on it's training data.\n",
    "\n",
    "### Temperature set to close to 1 is not a guarantee to get creative responses\n",
    "\n",
    "Let's try the same setting with a different user interaction: ***May the force be with***. A Temperature of 0.7 indicates to the model to be \"creative\" in responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "pwsh"
    },
    "polyglot_notebook": {
     "kernelName": "pwsh"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31;1m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\u001b[0m\n",
      "\u001b[31;1m                                 Dload  Upload   Total   Spent    Left  Speed\u001b[0m\n",
      "\u001b[31;1m\u001b[0m\n",
      "\u001b[31;1m  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\u001b[0m\n",
      "\u001b[31;1m100   667  100   282  100   385    380    519 --:--:-- --:--:-- --:--:--   902\u001b[0m\n",
      "you\n",
      "\u001b[31;1m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\u001b[0m\n",
      "\u001b[31;1m                                 Dload  Upload   Total   Spent    Left  Speed\u001b[0m\n",
      "\u001b[31;1m\u001b[0m\n",
      "\u001b[31;1m  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\u001b[0m\n",
      "\u001b[31;1m  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\u001b[0m\n",
      "\u001b[31;1m100   667  100   282  100   385    388    529 --:--:-- --:--:-- --:--:--   920\u001b[0m\n",
      "you\n",
      "\u001b[31;1m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\u001b[0m\n",
      "\u001b[31;1m                                 Dload  Upload   Total   Spent    Left  Speed\u001b[0m\n",
      "\u001b[31;1m\u001b[0m\n",
      "\u001b[31;1m  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\u001b[0m\n",
      "\u001b[31;1m  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\u001b[0m\n",
      "\u001b[31;1m100   667  100   282  100   385    367    501 --:--:-- --:--:-- --:--:--   871\u001b[0m\n",
      "you\n"
     ]
    }
   ],
   "source": [
    "$jsonPayload = @\"\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are an AI assistant that completes statements and phrases. You just finish the provided statement!\"\n",
    "        }, \n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"May the force be with\" \n",
    "        }\n",
    "    ], \n",
    "    \"max_tokens\": 800,\n",
    "    \"temperature\": 0.7,\n",
    "    \"stop\": [\".\"]\n",
    "}\n",
    "\"@\n",
    "\n",
    "for ($i=1; $i -le 3; $i++) {\n",
    "    $response = curl $url `\n",
    "    -H \"Content-Type: application/json\" `\n",
    "    -H \"api-key: $apiKey\" `\n",
    "    -d $jsonpayload \n",
    "    \n",
    "    Write-Host ($response | ConvertFrom-Json).choices.message.content \n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model responses 3 times with the same result because it's training data doesn't provided to many variations of ***May the force be with...*** even if we provide a Temperature closer to 1. Meaning providing a Temperature value close to 1 does not automatically ensure that different responses are created. It makes it more likely if in the models training data contained multiple variants to respond or complete. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TopP\n",
    "\n",
    " TopP can be used to achieve a similar outcome but it works differently. TopP is also a float value between 0 and 1 and it limits the amount of potential responses from a LLM to the request. \n",
    "\n",
    "Let's assume the LLM has 100 potential tokens to complete the response a TopP value of 0.3 instructs the model to consider just 30 percent of the potential completions. Providing 0 as TopP limits the potential responses to the top completion possibility.\n",
    "\n",
    "Let's take the first example with the simplified prompt ***Once upon a time***, providing a Temperature of 1 which indicates the model to be \"creative\" with completions but provide a TopP of 0 which indicates to the model to just use the top completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "dotnet_interactive": {
     "language": "pwsh"
    },
    "polyglot_notebook": {
     "kernelName": "pwsh"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31;1m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\u001b[0m\n",
      "\u001b[31;1m                                 Dload  Upload   Total   Spent    Left  Speed\u001b[0m\n",
      "\u001b[31;1m\u001b[0m\n",
      "\u001b[31;1m  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\u001b[0m\n",
      "\u001b[31;1m100   793  100   398  100   395    320    317  0:00:01  0:00:01 --:--:--   639\u001b[0m\n",
      "\u001b[31;1m100   793  100   398  100   395    319    317  0:00:01  0:00:01 --:--:--   638\u001b[0m\n",
      "in a far-off land, there lived a brave knight who embarked on a perilous quest to save the kingdom from an evil dragon\n",
      "\u001b[31;1m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\u001b[0m\n",
      "\u001b[31;1m                                 Dload  Upload   Total   Spent    Left  Speed\u001b[0m\n",
      "\u001b[31;1m\u001b[0m\n",
      "\u001b[31;1m  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\u001b[0m\n",
      "\u001b[31;1m100   793  100   398  100   395    322    319  0:00:01  0:00:01 --:--:--   644\u001b[0m\n",
      "\u001b[31;1m100   793  100   398  100   395    322    319  0:00:01  0:00:01 --:--:--   644\u001b[0m\n",
      "in a far-off land, there lived a brave knight who embarked on a perilous quest to save the kingdom from an evil dragon\n",
      "\u001b[31;1m  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\u001b[0m\n",
      "\u001b[31;1m                                 Dload  Upload   Total   Spent    Left  Speed\u001b[0m\n",
      "\u001b[31;1m\u001b[0m\n",
      "\u001b[31;1m  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\u001b[0m\n",
      "\u001b[31;1m  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\u001b[0m\n",
      "\u001b[31;1m100   395    0     0  100   395      0    289  0:00:01  0:00:01 --:--:--   290\u001b[0m\n",
      "\u001b[31;1m100   793  100   398  100   395    288    286  0:00:01  0:00:01 --:--:--   575\u001b[0m\n",
      "in a far-off land, there lived a brave knight who embarked on a perilous quest to save the kingdom from an evil dragon\n"
     ]
    }
   ],
   "source": [
    "$jsonPayload = @\"\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are an AI assistant that completes statements and phrases. You just finish the provided statement!\"\n",
    "        }, \n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Once upon a time\" \n",
    "        }\n",
    "    ], \n",
    "    \"max_tokens\": 800,\n",
    "    \"temperature\": 1,\n",
    "    \"top_p\": 0,\n",
    "    \"stop\": [\".\"]\n",
    "}\n",
    "\"@\n",
    "\n",
    "for ($i=1; $i -le 3; $i++) {\n",
    "    $response = curl $url `\n",
    "    -H \"Content-Type: application/json\" `\n",
    "    -H \"api-key: $apiKey\" `\n",
    "    -d $jsonpayload \n",
    "    \n",
    "    Write-Host ($response | ConvertFrom-Json).choices.message.content \n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Temperature and TopP influence the creativity and completions from the model and can be used in combination and provides some fine tuned control over the responses from the LLM. \n",
    "\n",
    "Sometimes influencing the completions from the LLM with one parameter is enough. Therefore as a rule of thumb: \n",
    "\n",
    "- Influencing responses using TopP -> Set Temperature to 1\n",
    "- Influencing responses using Temperature -> Set TopP to 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
