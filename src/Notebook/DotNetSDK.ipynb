{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI - Demystifying Temperature and TopP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this behavior in an example using the .NET OpenAI SDK and the Chat Completion API:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Create *OpenAIClient*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Azure.AI.OpenAi, 1.0.0-beta.5</span></li><li><span>azure.core, 1.30.0</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#r \"nuget: Azure.AI.OpenAi, 1.0.0-beta.5\"\n",
    "#r \"nuget: Azure.Core, 1.30.0\"\n",
    "\n",
    "using Azure;\n",
    "using Azure.AI.OpenAI; \n",
    "\n",
    "//Azure OpenAI model information \n",
    "Uri apiEndpoint = new Uri(\"provide your API endpoint\");\n",
    "string apiKey = \"provide your API key\"; \n",
    "string modelDeploymentName = \"provide your deployment name\"; \n",
    "\n",
    "//Create OpenAIClient & ChatApi\n",
    "AzureKeyCredential azureKeyCredential = new AzureKeyCredential(apiKey);\n",
    "OpenAIClient openAIClient = new OpenAIClient(\n",
    "    apiEndpoint,\n",
    "    azureKeyCredential\n",
    ");\n",
    "\n",
    "#!share --from c# openAIClient --as openAIClient \n",
    "#!share --from c# modelDeploymentName --as modelDeploymentName\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P.S.: The necessary Azure environment (Azure OpenAI, model deployment etc.) can be created using the provided [Azure CLI script](../CreateEnv/CreateEnv.azcli).The API endpoint, API key and model deployment name to run the code snippet are provided in environment variables:\n",
    "\n",
    "```azurecli\n",
    "$ENV:AZURE_OPENAI_ENDPOINT = $csEndpoint\n",
    "$ENV:AZURE_OPENAI_API_KEY = $csApiKey\n",
    "$ENV:AZURE_OPENAI_DEPLOYMENTNAME = $modelDeploymentName\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Call Azure Open AI chat API endpoint:\n",
    "\n",
    "The first call uses the following parameter:\n",
    "\n",
    "- **System Message:** \"You are an AI assistant that completes statements and phrases. You just finish the provided statement!\n",
    "- **User:** \"Once upon a time\"\n",
    "- **Temperature:** Set to 1 indicating to the model to be \"creative\" with responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div></div><div></div><div><strong>Installed Packages</strong><ul><li><span>Azure.AI.OpenAi, 1.0.0-beta.5</span></li></ul></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: there was a small village tucked away in the mountains\n",
      "Assistant: there was a small village located on the edge of a vast forest\n",
      "Assistant: there was a small town that held an annual talent show competition\n"
     ]
    }
   ],
   "source": [
    "#r \"nuget: Azure.AI.OpenAi, 1.0.0-beta.5\"\n",
    "\n",
    "using Azure.AI.OpenAI; \n",
    "\n",
    "string system = \"You are an AI assistant that completes statements and phrases. You just finish the provided statement!\";\n",
    "string user = \"Once upon a time\"; \n",
    "\n",
    "//Compose Chat\n",
    "ChatCompletionsOptions chatCompletionsOptions = new ChatCompletionsOptions();\n",
    "chatCompletionsOptions.Messages.Add(new ChatMessage(ChatRole.System, instructions));\n",
    "chatCompletionsOptions.Messages.Add(new ChatMessage(ChatRole.User, user));\n",
    "\n",
    "//Request Properties\n",
    "chatCompletionsOptions.Temperature = 1;  \n",
    "chatCompletionsOptions.StopSequences.Add(\".\");\n",
    "\n",
    "//Call OpenAI\n",
    "for (int i= 0; i<3; i++) {\n",
    "    Response<ChatCompletions> response = await openAIClient.GetChatCompletionsAsync(\n",
    "        modelDeploymentName,\n",
    "        chatCompletionsOptions);\n",
    "    ChatCompletions completions = response.Value;\n",
    "    Console.WriteLine($\"Assistant: {completions.Choices[0].Message.Content}\".Trim());\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different responses are created. GPT models are trained on large and diverse datasets. Meaning there are plenty of possible completions to the above simple ***Once upon a time*** user interaction. Hence the different responses from the LLM.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temperature & TopP\n",
    "\n",
    "By providing Temperature and/or TopP to the model the variability of responses can be influenced. Let's first have a look to Temperature\n",
    "\n",
    "### Temperature\n",
    "\n",
    "Temperature is a float value with a range between 0 and 1. Where 0 indicates to the model to be more deterministic meaning less variable response should be created. 1 indicates to the model that it can respond with more \"creativity\" and be less deterministic.\n",
    "\n",
    "Let's re-run the example with a Temperature of 0 to indicate to the model to be more deterministic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: there was a small village nestled in the heart of a dense forest\n",
      "Assistant: there was a small village nestled in the heart of a dense forest\n",
      "Assistant: there was a small village nestled in the heart of a dense forest\n"
     ]
    }
   ],
   "source": [
    "//Request Properties\n",
    "chatCompletionsOptions.Temperature = 0;  \n",
    "chatCompletionsOptions.StopSequences.Add(\".\");\n",
    "\n",
    "//Call OpenAI\n",
    "for (int i= 0; i<3; i++) {\n",
    "    Response<ChatCompletions> response = await openAIClient.GetChatCompletionsAsync(\n",
    "        modelDeploymentName,\n",
    "        chatCompletionsOptions);\n",
    "    ChatCompletions completions = response.Value;\n",
    "    Console.WriteLine($\"Assistant: {completions.Choices[0].Message.Content}\".Trim());\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen above Temperature controls the variability of the models responses based on it's training data.\n",
    "\n",
    "### Temperature set to close to 1 is not a guarantee to get creative responses\n",
    "\n",
    "Let's try the same setting with a different user interaction: ***May the force be with***. A Temperature of 0.7 indicates to the model to be \"creative\" in responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: you\n",
      "Assistant: you\n",
      "Assistant: you\n"
     ]
    }
   ],
   "source": [
    "user = \"May the force be with \";\n",
    "\n",
    "//Request Properties\n",
    "chatCompletionsOptions.Temperature = (float)0.7;  \n",
    "chatCompletionsOptions.StopSequences.Add(\".\");\n",
    "chatCompletionsOptions.Messages.Add(new ChatMessage(ChatRole.User, user));\n",
    "\n",
    "//Call OpenAI\n",
    "for (int i= 0; i<3; i++) {\n",
    "    Response<ChatCompletions> response = await openAIClient.GetChatCompletionsAsync(\n",
    "        modelDeploymentName,\n",
    "        chatCompletionsOptions);\n",
    "    ChatCompletions completions = response.Value;\n",
    "    Console.WriteLine($\"Assistant: {completions.Choices[0].Message.Content}\".Trim());\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model responses 3 times with the same result because it's training data doesn't provided to many variations of ***May the force be with...*** even if we provide a Temperature closer to 1. Meaning providing a Temperature value close to 1 does not automatically ensure that different responses are created. It makes it more likely if in the models training data contained multiple variants to respond or complete. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TopP\n",
    "\n",
    " TopP can be used to achieve a similar outcome but it works differently. TopP is also a float value between 0 and 1 and it limits the amount of potential responses from a LLM to the request. \n",
    "\n",
    "Let's assume the LLM has 100 potential tokens to complete the response a TopP value of 0.3 instructs the model to consider just 30 percent of the potential completions. Providing 0 as TopP limits the potential responses to the top completion possibility.\n",
    "\n",
    "Let's take the first example with the simplified prompt ***Once upon a time***, providing a Temperature of 1 which indicates the model to be \"creative\" with completions but provide a TopP of 0 which indicates to the model to just use the top completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "dotnet_interactive": {
     "language": "csharp"
    },
    "polyglot_notebook": {
     "kernelName": "csharp"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", in a far-off kingdom, there lived a beautiful princess named Isabella\n",
      ", in a far-off kingdom, there lived a beautiful princess named Isabella\n",
      "in a far-off kingdom, there lived a beautiful princess named Isabella\n"
     ]
    }
   ],
   "source": [
    "user = \"Once upon a time\"; \n",
    "\n",
    "//Prompt \n",
    "chatCompletionsOptions.Messages.Add(new ChatMessage(ChatRole.User, user));\n",
    "\n",
    "//Request Properties\n",
    "chatCompletionsOptions.Temperature = (float)1;  \n",
    "chatCompletionsOptions.NucleusSamplingFactor = (float)0;\n",
    "chatCompletionsOptions.StopSequences.Add(\".\");\n",
    "\n",
    "//Call OpenAI\n",
    "for (int i= 0; i<3; i++) {\n",
    "    Response<ChatCompletions> response = await openAIClient.GetChatCompletionsAsync(\n",
    "        modelDeploymentName,\n",
    "        chatCompletionsOptions);\n",
    "    ChatCompletions completions = response.Value;\n",
    "    Console.WriteLine(completions.Choices[0].Message.Content);\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Temperature and TopP influence the creativity and completions from the model and can be used in combination and provides some fine tuned control over the responses from the LLM. \n",
    "\n",
    "Sometimes influencing the completions from the LLM with one parameter is enough. Therefore as a rule of thumb: \n",
    "\n",
    "- Influencing responses using TopP -> Set Temperature to 1\n",
    "- Influencing responses using Temperature -> Set TopP to 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (C#)",
   "language": "C#",
   "name": ".net-csharp"
  },
  "language_info": {
   "name": "polyglot-notebook"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "languageName": "csharp",
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
